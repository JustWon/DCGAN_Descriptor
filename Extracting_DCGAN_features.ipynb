{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20171007실험\n",
    "\n",
    " - New College의 영상당 디스크립터 갯수 되는대로 많이(=SURF의 갯수만큼) 해서 실험\n",
    " - final_survey 데이터로 디스크립터 생성\n",
    " - DCGAN의 마지막 레이어 전체를 사용해서 디스크립터 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "generator/g_h0_lin/Matrix:0 (float32_ref 100x8192) [819200, bytes: 3276800]\n",
      "generator/g_h0_lin/bias:0 (float32_ref 8192) [8192, bytes: 32768]\n",
      "generator/g_bn0/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
      "generator/g_bn0/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
      "generator/g_h1/w:0 (float32_ref 5x5x256x512) [3276800, bytes: 13107200]\n",
      "generator/g_h1/biases:0 (float32_ref 256) [256, bytes: 1024]\n",
      "generator/g_bn1/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "generator/g_bn1/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
      "generator/g_h2/w:0 (float32_ref 5x5x128x256) [819200, bytes: 3276800]\n",
      "generator/g_h2/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "generator/g_bn2/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "generator/g_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
      "generator/g_h3/w:0 (float32_ref 5x5x64x128) [204800, bytes: 819200]\n",
      "generator/g_h3/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "generator/g_h4/w:0 (float32_ref 5x5x3x64) [4800, bytes: 19200]\n",
      "generator/g_h4/biases:0 (float32_ref 3) [3, bytes: 12]\n",
      "discriminator/d_h0_conv/w:0 (float32_ref 5x5x3x64) [4800, bytes: 19200]\n",
      "discriminator/d_h0_conv/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "discriminator/d_h1_conv/w:0 (float32_ref 5x5x64x128) [204800, bytes: 819200]\n",
      "discriminator/d_h1_conv/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "discriminator/d_bn1/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "discriminator/d_bn1/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
      "discriminator/d_h2_conv/w:0 (float32_ref 5x5x128x256) [819200, bytes: 3276800]\n",
      "discriminator/d_h2_conv/biases:0 (float32_ref 256) [256, bytes: 1024]\n",
      "discriminator/d_bn2/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "discriminator/d_bn2/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
      "discriminator/d_h3_conv/w:0 (float32_ref 5x5x256x512) [3276800, bytes: 13107200]\n",
      "discriminator/d_h3_conv/biases:0 (float32_ref 512) [512, bytes: 2048]\n",
      "discriminator/d_bn3/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
      "discriminator/d_bn3/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
      "discriminator/d_h4_lin/Matrix:0 (float32_ref 8192x1) [8192, bytes: 32768]\n",
      "discriminator/d_h4_lin/bias:0 (float32_ref 1) [1, bytes: 4]\n",
      "Total size of variables: 9451908\n",
      "Total bytes of variables: 37807632\n",
      " [*] Reading checkpoints...\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/PatchofPlaces_128_64_64/DCGAN.model-2164002\n",
      " [*] Success to read DCGAN.model-2164002\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from model import DCGAN\n",
    "from utils import pp, visualize, to_json, show_all_variables\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import sys\n",
    "\n",
    "g_batch_size = 128\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_integer(\"epoch\", 25, \"Epoch to train [25]\")\n",
    "flags.DEFINE_float(\"learning_rate\", 0.0002, \"Learning rate of for adam [0.0002]\")\n",
    "flags.DEFINE_float(\"beta1\", 0.5, \"Momentum term of adam [0.5]\")\n",
    "flags.DEFINE_integer(\"train_size\", np.inf, \"The size of train images [np.inf]\")\n",
    "flags.DEFINE_integer(\"batch_size\", g_batch_size, \"The size of batch images [64]\")\n",
    "flags.DEFINE_integer(\"input_height\", 64, \"The size of image to use (will be center cropped). [108]\")\n",
    "flags.DEFINE_integer(\"input_width\", None,\n",
    "                     \"The size of image to use (will be center cropped). If None, same value as input_height [None]\")\n",
    "flags.DEFINE_integer(\"output_height\", 64, \"The size of the output images to produce [64]\")\n",
    "flags.DEFINE_integer(\"output_width\", None,\n",
    "                     \"The size of the output images to produce. If None, same value as output_height [None]\")\n",
    "flags.DEFINE_string(\"dataset\", \"PatchofPlaces\", \"The name of dataset [celebA, mnist, lsun]\")\n",
    "flags.DEFINE_string(\"input_fname_pattern\", \"*/*.jpg\", \"Glob pattern of filename of input images [*]\")\n",
    "flags.DEFINE_string(\"checkpoint_dir\", \"checkpoint\", \"Directory name to save the checkpoints [checkpoint]\")\n",
    "flags.DEFINE_string(\"sample_dir\", \"samples\", \"Directory name to save the image samples [samples]\")\n",
    "flags.DEFINE_boolean(\"train\", False, \"True for training, False for testing [False]\")\n",
    "flags.DEFINE_boolean(\"crop\", False, \"True for training, False for testing [False]\")\n",
    "flags.DEFINE_boolean(\"visualize\", False, \"True for visualizing, False for nothing [False]\")\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "\n",
    "pp.pprint(flags.FLAGS.__flags)\n",
    "\n",
    "if FLAGS.input_width is None:\n",
    "    FLAGS.input_width = FLAGS.input_height\n",
    "if FLAGS.output_width is None:\n",
    "    FLAGS.output_width = FLAGS.output_height\n",
    "\n",
    "if not os.path.exists(FLAGS.checkpoint_dir):\n",
    "    os.makedirs(FLAGS.checkpoint_dir)\n",
    "if not os.path.exists(FLAGS.sample_dir):\n",
    "    os.makedirs(FLAGS.sample_dir)\n",
    "\n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "run_config = tf.ConfigProto()\n",
    "run_config.gpu_options.allow_growth = True\n",
    "\n",
    "\n",
    "sess = tf.Session(config=run_config)\n",
    "\n",
    "dcgan = DCGAN(\n",
    "    sess,\n",
    "    input_width=FLAGS.input_width,\n",
    "    input_height=FLAGS.input_height,\n",
    "    output_width=FLAGS.output_width,\n",
    "    output_height=FLAGS.output_height,\n",
    "    batch_size=FLAGS.batch_size,\n",
    "    sample_num=FLAGS.batch_size,\n",
    "    dataset_name=FLAGS.dataset,\n",
    "    input_fname_pattern=FLAGS.input_fname_pattern,\n",
    "    crop=FLAGS.crop,\n",
    "    checkpoint_dir=FLAGS.checkpoint_dir,\n",
    "    sample_dir=FLAGS.sample_dir)\n",
    "\n",
    "show_all_variables()\n",
    "\n",
    "if not dcgan.load(FLAGS.checkpoint_dir)[0]:\n",
    "    raise Exception(\"[!] Train a model first, then run test mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer_extraction(dcgan, file_names):\n",
    "    return dcgan.get_feature(FLAGS, file_names)\n",
    "    \n",
    "def maxpooling(disc):\n",
    "    pooling = tf.nn.max_pool(disc[3],ksize=[1,1,1,1], strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "    pool_result = sess.run(pooling)\n",
    "\n",
    "#     for idx in range(4):\n",
    "#         print(idx, maxpool_result[idx].shape)\n",
    "\n",
    "    return pool_result\n",
    "\n",
    "def flatten(disc):\n",
    "#     flatten = [\n",
    "#         tf.reshape(disc[i],[g_batch_size, -1])\n",
    "#         for i in range(4)\n",
    "#     ]\n",
    "    flatten = tf.reshape(disc, [g_batch_size, -1])\n",
    "    flatten_result = sess.run(flatten)\n",
    "    \n",
    "    return flatten_result\n",
    "\n",
    "def concat(disc):\n",
    "    concat = tf.concat(disc,1)\n",
    "\n",
    "    concat_result = sess.run(concat)\n",
    "    \n",
    "    return concat_result\n",
    "\n",
    "def feature_ext_GAN(file_names):\n",
    "    \n",
    "    ret = layer_extraction(dcgan, file_names)\n",
    "    ret = maxpooling(ret)\n",
    "    ret = flatten(ret)\n",
    "    ret = concat(ret)\n",
    "        \n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ~ 50\n",
      "total: 21140\n",
      "[ '/media/dongwonshin/Ubuntu Data/Datasets/FAB-MAP/Image Data/New College ManualLC/patches/survey_final/0819/00000000.jpg'\n",
      " '/media/dongwonshin/Ubuntu Data/Datasets/FAB-MAP/Image Data/New College ManualLC/patches/survey_final/0819/00000001.jpg'\n",
      " '/media/dongwonshin/Ubuntu Data/Datasets/FAB-MAP/Image Data/New College ManualLC/patches/survey_final/0819/00000002.jpg'\n",
      " ...,\n",
      " '/media/dongwonshin/Ubuntu Data/Datasets/FAB-MAP/Image Data/New College ManualLC/patches/survey_final/1932/00001354.jpg'\n",
      " '/media/dongwonshin/Ubuntu Data/Datasets/FAB-MAP/Image Data/New College ManualLC/patches/survey_final/1932/00001355.jpg'\n",
      " '/media/dongwonshin/Ubuntu Data/Datasets/FAB-MAP/Image Data/New College ManualLC/patches/survey_final/1932/00001356.jpg']\n",
      ".....................................................................................................................................................................50 ~ 100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7ec197b9279e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mfile_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mfile_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "# patch_path =\"/media/dongwonshin/Ubuntu Data/Datasets/Places365/Large_images/val_large/patches\"\n",
    "# data = glob(\"%s/Places365_val_%08d/*.jpg\" % (patch_path, idx))\n",
    "# output_filename = '/media/dongwonshin/Ubuntu Data/Datasets/Places365/Large_images/val_large/descs/20170702/' + (name.split('/')[-2])+'.desc'\n",
    "\n",
    "for term in np.arange(0,10):\n",
    "    print('%d ~ %d' % (50*term,50*(term+1)))\n",
    "    \n",
    "    disc_list = []\n",
    "    batch_list = []\n",
    "    file_names = []\n",
    "    \n",
    "    patch_dirs_path = \"/media/dongwonshin/Ubuntu Data/Datasets/FAB-MAP/Image Data/New College ManualLC/patches/survey_final/\"\n",
    "#     patch_dirs_path = \"/media/dongwonshin/Ubuntu Data/Datasets/FAB-MAP/Image Data/City Centre ManualLC/patches/online_survey/\"\n",
    "#     patch_dirs_path = \"/media/dongwonshin/Ubuntu Data/Datasets/Places365/Large_images/val_large (36500)/patches/vanilla network/\"\n",
    "    patch_dirs = sorted(glob(\"%s/*/\" % (patch_dirs_path)))\n",
    "    patch_dirs = patch_dirs[term*50:(term+1)*50]\n",
    "    \n",
    "    for patch_dir in patch_dirs:\n",
    "        data = sorted(glob(\"%s/*.jpg\" % (patch_dir)))\n",
    "        file_names.append(data)\n",
    "\n",
    "    file_names=np.concatenate(file_names)\n",
    "    print('total:',len(file_names))\n",
    "    print(file_names)\n",
    "\n",
    "    idx = 0\n",
    "    for idx in range(0, len(file_names)-g_batch_size,g_batch_size):\n",
    "        batch_files = file_names[idx: idx+g_batch_size]\n",
    "\n",
    "        disc = feature_ext_GAN(batch_files)\n",
    "        disc_list.append(disc)\n",
    "        batch_list.append(batch_files)\n",
    "        sys.stdout.write('.')\n",
    "\n",
    "    final_disc_list = np.concatenate(disc_list)\n",
    "    final_batch_list = np.concatenate(batch_list)\n",
    "    \n",
    "    for idx, name in enumerate(final_batch_list):\n",
    "        output_filename = '/media/dongwonshin/Ubuntu Data/Datasets/FAB-MAP/Image Data/New College ManualLC/descs/survey_final_8192dim/' + (name.split('/')[-2])+'.desc'\n",
    "#         output_filename = '/media/dongwonshin/Ubuntu Data/Datasets/FAB-MAP/Image Data/City Centre ManualLC/descs/all_desc/' + (name.split('/')[-2])+'.desc'\n",
    "#         output_filename = '/media/dongwonshin/Ubuntu Data/Datasets/Places365/Large_images/val_large (36500)/descs/1000desc/' + (name.split('/')[-2])+'.desc'\n",
    "        with open(output_filename,'at') as fp:\n",
    "            for v in final_disc_list[idx]:\n",
    "                fp.write('%f ' % v)\n",
    "            fp.write('\\n')\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
