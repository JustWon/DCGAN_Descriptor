{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "generator/g_h0_lin/Matrix:0 (float32_ref 100x8192) [819200, bytes: 3276800]\n",
      "generator/g_h0_lin/bias:0 (float32_ref 8192) [8192, bytes: 32768]\n",
      "generator/g_bn0/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
      "generator/g_bn0/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
      "generator/g_h1/w:0 (float32_ref 5x5x256x512) [3276800, bytes: 13107200]\n",
      "generator/g_h1/biases:0 (float32_ref 256) [256, bytes: 1024]\n",
      "generator/g_bn1/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "generator/g_bn1/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
      "generator/g_h2/w:0 (float32_ref 5x5x128x256) [819200, bytes: 3276800]\n",
      "generator/g_h2/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "generator/g_bn2/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "generator/g_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
      "generator/g_h3/w:0 (float32_ref 5x5x64x128) [204800, bytes: 819200]\n",
      "generator/g_h3/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "generator/g_h4/w:0 (float32_ref 5x5x3x64) [4800, bytes: 19200]\n",
      "generator/g_h4/biases:0 (float32_ref 3) [3, bytes: 12]\n",
      "discriminator/d_h0_conv/w:0 (float32_ref 5x5x3x64) [4800, bytes: 19200]\n",
      "discriminator/d_h0_conv/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "discriminator/d_h1_conv/w:0 (float32_ref 5x5x64x128) [204800, bytes: 819200]\n",
      "discriminator/d_h1_conv/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "discriminator/d_bn1/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "discriminator/d_bn1/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
      "discriminator/d_h2_conv/w:0 (float32_ref 5x5x128x256) [819200, bytes: 3276800]\n",
      "discriminator/d_h2_conv/biases:0 (float32_ref 256) [256, bytes: 1024]\n",
      "discriminator/d_bn2/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "discriminator/d_bn2/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
      "discriminator/d_h3_conv/w:0 (float32_ref 5x5x256x512) [3276800, bytes: 13107200]\n",
      "discriminator/d_h3_conv/biases:0 (float32_ref 512) [512, bytes: 2048]\n",
      "discriminator/d_bn3/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
      "discriminator/d_bn3/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
      "discriminator/d_h4_lin/Matrix:0 (float32_ref 8192x1) [8192, bytes: 32768]\n",
      "discriminator/d_h4_lin/bias:0 (float32_ref 1) [1, bytes: 4]\n",
      "Total size of variables: 9451908\n",
      "Total bytes of variables: 37807632\n",
      " [*] Reading checkpoints...\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/PatchofPlaces_128_64_64/DCGAN.model-2164002\n",
      " [*] Success to read DCGAN.model-2164002\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from model import DCGAN\n",
    "from utils import pp, visualize, to_json, show_all_variables\n",
    "\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import sys\n",
    "\n",
    "g_batch_size = 128\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_integer(\"epoch\", 25, \"Epoch to train [25]\")\n",
    "flags.DEFINE_float(\"learning_rate\", 0.0002, \"Learning rate of for adam [0.0002]\")\n",
    "flags.DEFINE_float(\"beta1\", 0.5, \"Momentum term of adam [0.5]\")\n",
    "flags.DEFINE_integer(\"train_size\", np.inf, \"The size of train images [np.inf]\")\n",
    "flags.DEFINE_integer(\"batch_size\", g_batch_size, \"The size of batch images [64]\")\n",
    "flags.DEFINE_integer(\"input_height\", 64, \"The size of image to use (will be center cropped). [108]\")\n",
    "flags.DEFINE_integer(\"input_width\", None, \"The size of image to use (will be center cropped). If None, same value as input_height [None]\")\n",
    "flags.DEFINE_integer(\"output_height\", 64, \"The size of the output images to produce [64]\")\n",
    "flags.DEFINE_integer(\"output_width\", None, \"The size of the output images to produce. If None, same value as output_height [None]\")\n",
    "flags.DEFINE_string(\"dataset\", \"PatchofPlaces\", \"The name of dataset [celebA, mnist, lsun]\")\n",
    "flags.DEFINE_string(\"input_fname_pattern\", \"*/*.jpg\", \"Glob pattern of filename of input images [*]\")\n",
    "flags.DEFINE_string(\"checkpoint_dir\", \"checkpoint\", \"Directory name to save the checkpoints [checkpoint]\")\n",
    "flags.DEFINE_string(\"sample_dir\", \"samples\", \"Directory name to save the image samples [samples]\")\n",
    "flags.DEFINE_boolean(\"train\", False, \"True for training, False for testing [False]\")\n",
    "flags.DEFINE_boolean(\"crop\", False, \"True for training, False for testing [False]\")\n",
    "flags.DEFINE_boolean(\"visualize\", False, \"True for visualizing, False for nothing [False]\")\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "pp.pprint(flags.FLAGS.__flags)\n",
    "\n",
    "if FLAGS.input_width is None:\n",
    "    FLAGS.input_width = FLAGS.input_height\n",
    "if FLAGS.output_width is None:\n",
    "    FLAGS.output_width = FLAGS.output_height\n",
    "\n",
    "if not os.path.exists(FLAGS.checkpoint_dir):\n",
    "    os.makedirs(FLAGS.checkpoint_dir)\n",
    "if not os.path.exists(FLAGS.sample_dir):\n",
    "    os.makedirs(FLAGS.sample_dir)\n",
    "\n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "run_config = tf.ConfigProto()\n",
    "run_config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.Session(config=run_config)\n",
    "\n",
    "dcgan = DCGAN(\n",
    "    sess,\n",
    "    input_width=FLAGS.input_width,\n",
    "    input_height=FLAGS.input_height,\n",
    "    output_width=FLAGS.output_width,\n",
    "    output_height=FLAGS.output_height,\n",
    "    batch_size=FLAGS.batch_size,\n",
    "    sample_num=FLAGS.batch_size,\n",
    "    dataset_name=FLAGS.dataset,\n",
    "    input_fname_pattern=FLAGS.input_fname_pattern,\n",
    "    crop=FLAGS.crop,\n",
    "    checkpoint_dir=FLAGS.checkpoint_dir,\n",
    "    sample_dir=FLAGS.sample_dir,\n",
    "    mode=\"feature_extraction\")\n",
    "\n",
    "\n",
    "show_all_variables()\n",
    "\n",
    "if not dcgan.load(FLAGS.checkpoint_dir)[0]:\n",
    "    raise Exception(\"[!] Train a model first, then run test mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions for the DCGAN features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer_extraction(dcgan, file_names):\n",
    "    return dcgan.get_feature(FLAGS, file_names)\n",
    "    \n",
    "def maxpooling(disc):\n",
    "    pooling = tf.nn.max_pool(disc[3],ksize=[1,4,4,1], strides=[1,4,4,1],padding='SAME')\n",
    "    pool_result = sess.run(pooling)\n",
    "    return pool_result\n",
    "\n",
    "def flatten(disc):\n",
    "    flatten = tf.reshape(disc, [g_batch_size, -1])\n",
    "    flatten_result = sess.run(flatten)\n",
    "    \n",
    "    return flatten_result\n",
    "\n",
    "def concat(disc):\n",
    "    concat = tf.concat(disc,1)\n",
    "    concat_result = sess.run(concat)\n",
    "    return concat_result\n",
    "\n",
    "def feature_extraction_DCGAN(file_names):\n",
    "    \n",
    "    ret = layer_extraction(dcgan, file_names)\n",
    "    ret = maxpooling(ret)\n",
    "    ret = flatten(ret)\n",
    "    ret = concat(ret)\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_path = \"/media/dongwonshin/Ubuntu Data/Datasets/FAB-MAP/Image Data/New College ManualLC/patches/survey_final/\"\n",
    "output_path = '/media/dongwonshin/Ubuntu Data/Datasets/FAB-MAP/Image Data/New College ManualLC/descs/test/'\n",
    "\n",
    "for term in np.arange(0,10):\n",
    "    print('For %d ~ %d images,' % (50*term,50*(term+1)))\n",
    "    \n",
    "    disc_list = []\n",
    "    batch_list = []\n",
    "    file_names = []\n",
    "    \n",
    "    patch_dirs = sorted(glob(\"%s/*/\" % (input_path)))\n",
    "    patch_dirs = patch_dirs[term*50:(term+1)*50]\n",
    "    \n",
    "    for patch_dir in patch_dirs:\n",
    "        data = sorted(glob(\"%s/*.jpg\" % (patch_dir)))\n",
    "        file_names.append(data)\n",
    "\n",
    "    file_names=np.concatenate(file_names)\n",
    "    print('The total number of patches:',len(file_names))\n",
    "    print(file_names)\n",
    "\n",
    "    idx = 0\n",
    "    for idx in range(0, len(file_names)-g_batch_size,g_batch_size):\n",
    "        batch_files = file_names[idx: idx+g_batch_size]\n",
    "\n",
    "        disc = feature_extraction_DCGAN(batch_files)\n",
    "        disc_list.append(disc)\n",
    "        batch_list.append(batch_files)\n",
    "        sys.stdout.write('.')\n",
    "\n",
    "    final_disc_list = np.concatenate(disc_list)\n",
    "    final_batch_list = np.concatenate(batch_list)\n",
    "    \n",
    "    for idx, name in enumerate(final_batch_list):\n",
    "        output_filename = output_path + (name.split('/')[-2])+'.desc'\n",
    "        with open(output_filename,'at') as fp:\n",
    "            for v in final_disc_list[idx]:\n",
    "                fp.write('%f ' % v)\n",
    "            fp.write('\\n')\n",
    "\n",
    "print('done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
